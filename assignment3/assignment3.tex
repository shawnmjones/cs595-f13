\documentclass[letterpaper,11pt]{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[super]{nth}
\usepackage[hyphens]{url}

\lstset{
	basicstyle=\footnotesize,
	breaklines=true,
}

\begin{document}

\begin{titlepage}

\begin{center}

\Huge{Assignment 3}

\Large{CS 595:  Introduction to Web Science}

\Large{Fall 2013}

\Large{Shawn M. Jones}

\Large Finished on \today

\end{center}

\end{titlepage}

\newpage
\section*{1}

\subsection*{Question}

\begin{verbatim}
1.  Download the 1000 URIs from assignment #2.  "curl", "wget", or
"lynx" are all good candidate programs to use.  We want just the
raw HTML, not the images, stylesheets, etc.

from the command line:

% curl http://www.cnn.com/ > www.cnn.com

% wget -O www.cnn.com http://www.cnn.com/

% lynx -source http://www.cnn.com/ > www.cnn.com

"www.cnn.com" is just an example output file name, keep in mind
that the shell will not like some of the characters that can occur
in URIs (e.g., "?", "&").  You might want to hash the URIs, like:

% echo -n "http://www.cs.odu.edu/show_features.shtml?72" | md5
41d5f125d13b4bb554e6e31b6b591eeb

("md5sum" on some machines; note the "-n" in echo -- this removes
the trailing newline.) 

Now use a tool to remove (most) of the HTML markup.  "lynx" will
do a fair job:

% lynx -dump -force_html www.cnn.com > www.cnn.com.processed

Use another (better) tool if you know of one.  Keep both files 
for each URI (i.e., raw HTML and processed). 
\end{verbatim}

\newpage
\subsection*{Answer}

Downloading the URIs was done relatively easily using the script shown in Listing \ref{lst:q1script}.

The script is run like so:
\begin{lstlisting}[frame=single]
./downloadURIs.sh urilist-final.txt shalist-final.txt collection failedfile.txt
\end{lstlisting}

The arguments to the script are:
\begin{enumerate}
\item file containing URIs, one per line (\verb+urilist-final.txt+)
\item file used to associate SHA-1 hashes with each URI, written to as each URI is processed, with the SHA-1 hashes being part of the filenames of the downloaded representations of each URI (\verb+shalist-final.txt+)
\item directory name to store the downloaded representations (\verb+collection+)
\item file used to store URIs that fail to download (\verb+failedfile.txt+)
\end{enumerate}

Once the script is run, the directory used in the third argument (\verb+collection+) contains filenames, such as \verb+f8017e9dd34d714681d55693689736d5d3f56021.raw+ and \verb+f8017e9dd34d714681d55693689736d5d3f56021.processed+.  Files with a \verb+.raw+ extension contain the raw representation downloaded on line 32.  Files containing a \verb+.processed+ contain the representation stripped of any HTML, done on line 42.

The file used in the second argument (\verb+shalist-final.txt+) is used to associate these filenames to URIs (e.g. \verb+f8017e9dd34d714681d55693689736d5d3f56021+ corresponds to \verb+http://www.cnn.com/2013/07/19/politics/obama-zimmerman/+).

The \verb+set -e+ on line 4 causes the script to exit at the first sign of trouble.  This is a simple error-handling mechanism to place in Bourne shell scripts.

Unfortunately, the curl call on line 32 produced the following errors for several URIs:
\begin{itemize}
\item \verb+curl: (7) couldn't connect to host+
\item \verb+curl: (18) transfer closed with outstanding read data remaining+
\item \verb+curl: (52) Empty reply from server+
\end{itemize}

This is why lines 35-38 exist.  Line 31 undoes the error-handling mechanism so we can deal with the failed status on lines 33 and 35.  Any URI that fails to download the first time is stored in a \emph{failed file} (the fourth argument) which can then be fed through the script again, as the first argument, to (hopefully) successfully download the representations at a later time.  Fortunately, only one re-run was required.

\newpage
\lstinputlisting[language=bash, frame=single, caption={Bourne-Again Shell Program for downloading URIs from Assignment 2}, label=lst:q1script, captionpos=b, numbers=left, showspaces=false, showstringspaces=false, basicstyle=\footnotesize]{q1/downloadURIs.sh}

\newpage
\section*{2}

\subsection*{Question}

\begin{verbatim}
2.  Choose a query term (e.g., "shadow") that is not a stop words
(see week 4 slides) and not HTML markup from step 1 (e.g., "http")
that matches at least 10 documents (hint: use "grep" on the processed
files).  If the term is present in more than 10 documents, choose
any 10 from your list.  (If you do not end up with a list of 10
URIs, you've done something wrong).

As per the example in the week 4 slides, compute TFIDF values for
the term in each of the 10 documents and create a table with the
TF, IDF, and TFIDF values, as well as the corresponding URIs.  The
URIs will be ranked in decreasing order by TFIDF values.  For
example:

Table 1. 10 Hits for the term "shadow", ranked by TFIDF.

TFIDF	TF	IDF	URI
-----	--	---	---
0.150	0.014	10.680	http://foo.com/
0.044	0.008	 5.510	http://bar.com/


You can use Google or Bing for the DF estimation.  To count the
number of words in the processed document (i.e., the deonminator
for TF), you can use "wc":

% wc -w www.cnn.com.processed
    2370 www.cnn.com.processed

It won't be completely accurate, but it will be probably be
consistently inaccurate across all files.  You can use more 
accurate methods if you'd like.  

Don't forget the log base 2 for IDF, and mind your significant
digits!
\end{verbatim}

\newpage
\subsection*{Answer}

Searching for the term \emph{football} yielded quite a few results.  This was done with the following command:

\begin{lstlisting}[frame=single]
grep -i football *.processed | awk -F: '{ print $1 }' | sort | uniq -c | sort -rn | head -n 10
\end{lstlisting}

The top ten in term frequency were chosen for this exercise.

\begin{table}
\begin{tabular}{ | l | l | l | p{8.0cm} | }
\hline
\textbf{TFIDF} & \textbf{TF} & \textbf{IDF} & \textbf{URI} \\
\hline
0 & 0 & 0 & \url{http://www.dailykos.com/story/2013/06/18/1216969/-D-C-Football?utm_source=twitterfeed&utm_medium=twitter&utm_campaign=Feed%3A+dailykos%2Findex+%28Daily+Kos%29} \\
\hline
0 & 0 & 0 & \url{http://gif.mocksession.com/2013/02/rubio-is-thirsty/} \\
\hline
0 & 0 & 0 & \url{http://www.dailykos.com/story/2013/05/09/1207970/-Agreeing-with-McCain-on-Cable-bill?utm_source=twitterfeed&utm_medium=twitter&utm_campaign=Feed%3A+dailykos%2Findex+%28Daily+Kos%29} \\
\hline
0 & 0 & 0 & \url{http://www.tampabay.com/news/politics/national/mitt-romney-is-republican-partys-nominee-but-not-the-standard-bearer/1248507} \\
\hline
0 & 0 & 0 & \url{http://www.cnn.com/2013/04/23/justice/ohio-steubenville-coach/index.html?hpt=hp_t3} \\
\hline
0 & 0 & 0 & \url{http://host.madison.com/wsj/news/local/crime_and_courts/appeals-court-reverses-federal-judge-s-decision-upholds-collective-bargaining/article_c08d81f6-61a3-11e2-8ab7-001a4bcf887a.html} \\
\hline
0 & 0 & 0 & \url{http://bleacherreport.com/articles/1699257-major-league-baseball-suspends-ryan-braun-for-remainder-of-2013-season} \\
\hline
0 & 0 & 0 & \url{http://concord-nh.patch.com/groups/politics-and-elections/p/rep-s-sieg-heil-causes-furor} \\
\hline
0 & 0 & 0 & \url{http://folksdresseduplikeeskimos.tumblr.com/} \\
\hline
0 & 0 & 0 & \url{http://www.freep.com/article/20121205/NEWS15/121205082/Michigan-Rick-Snyder-emergency-manager-law-repeal-Lansing} \\
\hline
\end{tabular}
\caption{Table of URIs, TF, IDF and TF*IDF containing the word \emph{football}}
\label{table:q2}
\end{table}

\begin{verbatim}

\end{verbatim}

\newpage
\section*{3}

\subsection*{Question}

\begin{verbatim}
3.  Now rank the same 10 URIs from question #2, but this time 
by their PageRank.  Use any of the free PR estimaters on the web,
such as:

http://www.prchecker.info/check_page_rank.php
http://www.seocentro.com/tools/search-engines/pagerank.html
http://www.checkpagerank.net/

If you use these tools, you'll have to do so by hand (they have
anti-bot captchas), but there is only 10.  Normalize the values
they give you to be from 0 to 1.0.  Use the same tool on all 10
(again, consistency is more important than accuracy).

Create a table similar to Table 1:

Table 2.  10 hits for the term "shadow", ranked by PageRank.

PageRank	URI
--------	---
0.9		http://bar.com/
0.5		http://foo.com/

Briefly compare and contrast the rankings produced in questions 2
and 3.
\end{verbatim}

\newpage
\subsection*{Answer}

\newpage
\section*{4}

\subsection*{Question}

\begin{verbatim}
====================================================
======Question 4 is for 3 points extra credit=======
====================================================

4.  Compute the Kendall Tau_b score for both lists (use "b" because
there will likely be tie values in the rankings).  Report both the
Tau value and the "p" value.

See: 
http://stackoverflow.com/questions/2557863/measures-of-association-in-r-kendalls-tau-b-and-tau-c
http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient#Tau-b
http://en.wikipedia.org/wiki/Correlation_and_dependence
\end{verbatim}

\newpage
\subsection*{Answer}

\end{document}